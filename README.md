# machine_learning_research
## Другий крок (покращення)
Модель, котра застосовувалась в статті, котру ми досліджуємо показала f1_score 99.4%(настільки гарний скор може бути за рахунок того, що в нас не настільки багато даних, як би хотілось), що власне є неймовірно гарним результатом. Теоретичним покращенням може слугувати нейрона мережа. В даному випадку потрібно застосовувати звичайні нейроні мережі, котрі складаються з Dense шарів і Dropout шарів. Не факт, що це зможе покращити модель, адже xgboost працює на базі дерев, і може набагато краще показати результат на даних, котрі в нас є, проте якщо взяти інші моделі машинного навчання(окрім нейроних мереж). В даному випадку ще можемо спробувати застосувати LightGBM, RandomForest. LightGBM також застосовує бустінг, але він може бути значно глибшим і з меншою кількістю розгалужень. Для більш кращого дослідження і перевірки ми можемо застосувати не усі фічі даного датасету, а по декілька(до прикладу спершу беремо якісь 3 або більше, і так далі) і для кожних з них будемо юзати різні моделі, щоб потім усереднити і дізнатись яка найбільш гарна. Бо проблема в тому, що велика кількість фічів була зібрана, але даних не надто багато.
Прикладом однієї з архітектур нейроної мережі може бути дана:
>* model = Sequential()
>* model.add(Input())
>* model.add(Dense(32, activation='relu'))
>* model.add(Dense(16, activation='relu'))
>* model.add(Dense(1, activation='sigmoid'))

Вона неймовірна банальна, але може бути початком, і вже далі від неї можна відштовхуватись.
Якщо говорити про оптимізацію в плані швидкості, то це не буде мати сенсу, адже наш даний варіант працює менше за секунду